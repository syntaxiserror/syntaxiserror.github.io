{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12d88a4b-44ee-4a21-9a20-92c23d5a294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import os \n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9e1c2ee-5b78-4b09-8c78-0f0ebfec9764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up an env\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11aaafc8-4ead-4691-ab63-95198df2647e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [ 0.1468074   0.81210583 -0.22318335 -1.3416575 ] score:  14.0\n",
      "State: [ 0.16244939  0.20797847 -0.2239363  -1.0172493 ] score:  49.0\n",
      "State: [ 0.19090658  1.5321656  -0.22390114 -2.4633987 ] score:  34.0\n",
      "State: [ 0.19771065  1.3233826  -0.21626817 -2.0756438 ] score:  25.0\n",
      "State: [-0.13281009 -0.81411016  0.23160198  1.6471293 ] score:  14.0\n",
      "State: [ 0.12601152  1.1783102  -0.2390844  -1.9981263 ] score:  14.0\n",
      "State: [-0.19127518 -1.1433169   0.21940987  1.83952   ] score:  14.0\n",
      "State: [-0.0679687  -0.256439    0.22187732  0.7916867 ] score:  17.0\n",
      "State: [-0.01962391 -0.5858201   0.22966625  1.5862293 ] score:  65.0\n"
     ]
    }
   ],
   "source": [
    "# Making a random action \n",
    "for i in range (1, 10):\n",
    "    env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, X, info = env.step(action)\n",
    "        score += reward\n",
    "    print(\"State:\", n_state, \"score: \", score)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42913aba-7cd5-4aa6-9e27-2e4b90fa80da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [ 0.1468074   0.81210583 -0.22318335 -1.3416575 ] score:  14.0\n",
      "State: [ 0.16244939  0.20797847 -0.2239363  -1.0172493 ] score:  49.0\n",
      "State: [ 0.19090658  1.5321656  -0.22390114 -2.4633987 ] score:  34.0\n",
      "State: [ 0.19771065  1.3233826  -0.21626817 -2.0756438 ] score:  25.0\n",
      "State: [-0.13281009 -0.81411016  0.23160198  1.6471293 ] score:  14.0\n",
      "State: [ 0.12601152  1.1783102  -0.2390844  -1.9981263 ] score:  14.0\n",
      "State: [-0.19127518 -1.1433169   0.21940987  1.83952   ] score:  14.0\n",
      "State: [-0.0679687  -0.256439    0.22187732  0.7916867 ] score:  17.0\n",
      "State: [-0.01962391 -0.5858201   0.22966625  1.5862293 ] score:  65.0\n"
     ]
    }
   ],
   "source": [
    "# Making a random action \n",
    "for i in range (1, 10):\n",
    "    env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, X, info = env.step(action)\n",
    "        score += reward\n",
    "    print(\"State:\", n_state, \"score: \", score)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73ced99c-4193-4716-8f23-6682167bf43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a directory for logs\n",
    "logs_path = os.path.join(\"Training\", \"Logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5057d73d-4990-4256-8048-8fd17ca5fdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Creating an agent\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env = DummyVecEnv([lambda : env])\n",
    "agent = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=logs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bc65d16-058e-4bda-a060-1d0fe0d33e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_3\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 40   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 49   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 42          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009076357 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.00114     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.09        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 49.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 43          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 141         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009292733 |\n",
      "|    clip_fraction        | 0.0633      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.666      |\n",
      "|    explained_variance   | 0.0986      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.3        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    value_loss           | 32.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 185         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010341128 |\n",
      "|    clip_fraction        | 0.0959      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.635      |\n",
      "|    explained_variance   | 0.243       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.9        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    value_loss           | 51.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 231         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010136226 |\n",
      "|    clip_fraction        | 0.0777      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.605      |\n",
      "|    explained_variance   | 0.32        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19.5        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 64.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 44           |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 275          |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061503416 |\n",
      "|    clip_fraction        | 0.0413       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.593       |\n",
      "|    explained_variance   | 0.284        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.1         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0099      |\n",
      "|    value_loss           | 66           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 44           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 320          |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033386182 |\n",
      "|    clip_fraction        | 0.0142       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.589       |\n",
      "|    explained_variance   | 0.378        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 33.4         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00617     |\n",
      "|    value_loss           | 61.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 44          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 364         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004863827 |\n",
      "|    clip_fraction        | 0.0416      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.57       |\n",
      "|    explained_variance   | 0.54        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.2        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00849    |\n",
      "|    value_loss           | 57.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 409         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009539673 |\n",
      "|    clip_fraction        | 0.0993      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.56       |\n",
      "|    explained_variance   | 0.589       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.8        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 58.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 45           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 453          |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072503076 |\n",
      "|    clip_fraction        | 0.0711       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.553       |\n",
      "|    explained_variance   | 0.238        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.3         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00707     |\n",
      "|    value_loss           | 46.4         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x25721dad488>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9829ea1a-0633-4f3c-a827-ebc2b5f550a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_Model')\n",
    "agent.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "257ee89f-af77-48c5-9386-88e7d1bc9bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del agent\n",
    "#agent = PPO.load(PPO_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9c55341-5f63-4889-aa43-a7e196722bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500.0, 0.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(agent, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e27c2-927f-4b8b-8b63-41dd2200ad01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing agent\n",
    "for i in range (1, 10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _ = agent.predict(obs)\n",
    "        obs, reward, done, X, info = env.step(action)\n",
    "        score += reward\n",
    "    print(\"State:\", n_state, \"score: \", score)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowenv",
   "language": "python",
   "name": "tensorflowenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
